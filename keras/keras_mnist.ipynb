{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEHCAYAAABhvpAKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN3tWtL8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a6bd13c7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def readfile():\n",
    "    with open(r'F:\\data\\mnist\\train-images.idx3-ubyte','rb') as f1:\n",
    "        buf1 = f1.read()\n",
    "    with open(r'F:\\data\\mnist\\train-labels.idx1-ubyte','rb') as f2:\n",
    "        buf2 = f2.read()\n",
    "    return buf1, buf2\n",
    "\n",
    "\n",
    "def get_image(buf1):\n",
    "    image_index = 0\n",
    "    image_index += struct.calcsize('>IIII')\n",
    "    im = []\n",
    "    for _ in range(60000):\n",
    "        temp = struct.unpack_from('>784B', buf1, image_index) # '>784B'的意思就是用大端法读取784个unsigned byte\n",
    "        im.append(np.reshape(temp,(28, 28)))\n",
    "        image_index += struct.calcsize('>784B')  # 每次增加784B\n",
    "    return im\n",
    "\n",
    "\n",
    "def get_label(buf2): # 得到标签数据\n",
    "    label_index = 0\n",
    "    label_index += struct.calcsize('>II')\n",
    "    return struct.unpack_from('>60000B', buf2, label_index)\n",
    "\n",
    "image_data, label_data = readfile()\n",
    "im = get_image(image_data)\n",
    "label = get_label(label_data)\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "title = u\"标签对应为：\"+ str(label[0])\n",
    "plt.title(title, fontproperties='SimHei')\n",
    "plt.imshow(im[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTestFile():\n",
    "    with open(r'F:\\data\\mnist\\t10k-images.idx3-ubyte','rb') as f1:\n",
    "        buf1 = f1.read()\n",
    "    with open(r'F:\\data\\mnist\\t10k-labels.idx1-ubyte','rb') as f2:\n",
    "        buf2 = f2.read()\n",
    "    return buf1, buf2\n",
    "\n",
    "def get_image_test(buf1):\n",
    "    image_index = 0\n",
    "    image_index += struct.calcsize('>IIII')\n",
    "    im = []\n",
    "    for _ in range(10000):\n",
    "        temp = struct.unpack_from('>784B', buf1, image_index) # '>784B'的意思就是用大端法读取784个unsigned byte\n",
    "        im.append(np.reshape(temp,(28,28)))\n",
    "        image_index += struct.calcsize('>784B')  # 每次增加784B\n",
    "    return im\n",
    "\n",
    "def get_label_test(buf2): # 得到标签数据\n",
    "    label_index = 0\n",
    "    label_index += struct.calcsize('>II')\n",
    "    return struct.unpack_from('>10000B', buf2, label_index)\n",
    "\n",
    "data_test, label_test = readTestFile()\n",
    "im_test = get_image_test(image_data)\n",
    "label_test = get_label_test(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array(im).reshape(60000,1,28,28)\n",
    "y = np.array(label).reshape((60000,1))\n",
    "x.shape\n",
    "\n",
    "x_test = np.array(im_test).reshape((10000,28,28))\n",
    "y_test = np.array(label_test).reshape((10000,1))\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 1 for 'conv2d_16/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,6].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    655\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_16/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,6].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-051df7634730>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# this applies 32 convolution filters of size 3x3 each.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   3162\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3163\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3164\u001b[1;33m         data_format='NHWC')\n\u001b[0m\u001b[0;32m   3165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m         op=op)\n\u001b[0m\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mwith_space_to_batch\u001b[1;34m(input, dilation_rate, padding, op, filter_shape, spatial_dims, data_format)\u001b[0m\n\u001b[0;32m    336\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dilation_rate must be positive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconst_rate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m   \u001b[1;31m# We have two padding contributions. The first is used for converting \"SAME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mop\u001b[1;34m(input_converted, _, padding)\u001b[0m\n\u001b[0;32m    662\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     return with_space_to_batch(\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_non_atrous_convolution\u001b[1;34m(input, filter, padding, data_format, strides, name)\u001b[0m\n\u001b[0;32m    129\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    132\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NDHWC\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[0;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[0;32m    398\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2631\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2632\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    596\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fire\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 1 for 'conv2d_16/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,6]."
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(6, (3, 3), activation='relu', input_shape=(1,28,28)))\n",
    "model.add(Conv2D(6, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(6, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(6, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
