#### 1.判别模型 生成模型
都属于有监督。
常见的生成方法：混合高斯模型、朴素贝叶斯法和隐形马尔科夫模型等，常见的判别方法：SVM、LR、NN、CRF。

* 生成模型的求解思路是：联合分布------->求解类别先验概率和类别条件概率

* 判别模型求解的思路是：条件分布------>模型参数后验概率最大------->（似然函数\cdot 参数先验）最大------->最大似然。

优缺点：

* 生成模型：优点：
	1. 生成给出的是联合分布，不仅能够由联合分布计算条件分布（反之则不行），还可以给出其他信息，比如可以使用来计算边缘分布。如果一个输入样本的边缘分布很小的话，那么可以认为学习出的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好，这也是所谓的outlier detection。
	2. 生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。
	3. 生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。

缺点：1. 天下没有免费午餐，联合分布是能提供更多的信息，但也需要更多的样本和更多计算，尤其是为了更准确估计类别条件分布，需要增加样本的数目，而且类别条件概率的许多信息是我们做分类用不到，因而如果我们只需要做分类任务，就浪费了计算资源。2）另外，实践中多数情况下判别模型效果更好。

* 判别模型：优点：1）与生成模型缺点对应，首先是节省计算资源，另外，需要的样本数量也少于生成模型。2）准确率往往较生成模型高。3）由于直接学习，而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。缺点：1）是没有生成模型的上述优点。

#### 2.分词方法
中文分词的基本方法可以分为基于语法规则的方法、基于词典的方法和基于统计的方法。
    基于语法规则的分词法基本思想是在分词的同时进行句法、语义分析, 利用句法信息和语义信息来进行词性标注, 以解决分词歧义现象。因为现有的语法知识、句法规则十分笼统、复杂, 基于语法和规则的分词法所能达到的精确度远远还不能令人满意, 目前这种分词系统应用较少。
    在基于词典的方法中，可以进一步分为最大匹配法，最大概率法，最短路径法等。最大匹配法指的是按照一定顺序选取字符串中的若干个字当做一个词，去词典中查找。根据扫描方式可细分为：正向最大匹配，反向最大匹配，双向最大匹配，最小切分。最大概率法指的是一个待切分的汉字串可能包含多种分词结果，将其中概率最大的那个作为该字串的分词结果。最短路径法指的是在词图上选择一条词数最少的路径。
    基于统计的分词法的基本原理是根据字符串在语料库中出现的统计频率来决定其是否构成词。词是字的组合，相邻的字同时出现的次数越多, 就越有可能构成一个词。因此字与字相邻共现的频率或概率能够较好的反映它们成为词的可信度。常用的方法有HMM（隐马尔科夫模型），MAXENT（最大熵模型），MEMM（最大熵隐马尔科夫模型），CRF（条件随机场）。

#### 3.CRF
HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。
    CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。

#### 4.过拟合
HMM模型是对转移概率和表现概率直接建模，统计共现概率。而MEMM模型是对转移概率和表现概率建立联合概率，统计时统计的是条件概率。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布。MEMM容易陷入局部最优，是因为MEMM只在局部做归一化。CRF模型中，统计了全局概率，在做归一化时，考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。
    CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息，特征设计灵活。CRF需要训练的参数更多，与MEMM和HMM相比，它存在训练代价大、复杂度高的缺点。

#### 4.序列问题
EM算法： 只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法
维特比算法： 用动态规划解决HMM的预测问题，不是参数估计
前向后向算法：用来算概率
极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数
注意的是在给定观测序列和对应的状态序列估计模型参数，可以利用极大似然发估计。如果给定观测序列，没有对应的状态序列，才用EM，将状态序列看不不可测的隐数据。

#### 5.逻辑回归
Logit回归目标函数是最大似然估计，Logit回归可以用于预测事件发生概率的大小，SVM目标是结构风险最小化。

Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。

先来说一下最大似然（MLE）和最大后验概率（MAP）这一对概念吧，这两个概念分别代表的是学术界两个派别，频率派和贝叶斯派的观点，最大似然估计（频率派）认为模型中的未知参数虽然未知，但是是一个常量（constant variable）,而最大后验概率（贝叶斯派）认为模型中的未知参数是一个随机变量（random variable），也是服从某种分布的,只不过这种分布不被人所认知，这是需要我们去求解的。

#### 6.线性分类器
线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

* 感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。

* 支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）

* Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。
根据两类样本一般类内密集，类间分离的特点，寻找线性分类器最佳的法线向量方向，使两类样本在该方向上的投影满足类内尽可能密集，类间尽可能分开。这种度量通过类内离散矩阵 Sw 和类间离散矩阵 Sb 实现。
